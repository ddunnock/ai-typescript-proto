{"version":3,"sources":["../src/local.ts"],"names":[],"mappings":";;;;;;AAMO,IAAM,aAAA,GAAN,cAA4B,YAAA,CAAa;AAAA,EACnC,IAAA,GAAO,OAAA;AAAA,EACP,OAAA;AAAA,EACD,MAAA;AAAA,EACA,OAAA;AAAA,EAER,YAAY,OAAA,EAAkD;AAC1D,IAAA,KAAA,EAAM;AACN,IAAA,MAAM,SAAS,SAAA,EAAU;AAEzB,IAAA,IAAA,CAAK,OAAA,GAAU,OAAA,EAAS,OAAA,IAAW,MAAA,CAAO,aAAA;AAC1C,IAAA,IAAA,CAAK,SAAS,YAAA,CAAa,EAAE,OAAA,EAAS,IAAA,CAAK,SAAS,CAAA;AACpD,IAAA,IAAA,CAAK,OAAA,GAAU,OAAA,EAAS,OAAA,IAAW,MAAA,CAAO,WAAA;AAAA,EAC9C;AAAA,EAEA,MAAM,WAAA,GAAgC;AAClC,IAAA,IAAI;AACA,MAAA,MAAM,WAAW,MAAM,KAAA,CAAM,CAAA,EAAG,IAAA,CAAK,OAAO,CAAA,SAAA,CAAW,CAAA;AACvD,MAAA,OAAO,QAAA,CAAS,EAAA;AAAA,IACpB,CAAA,CAAA,MAAQ;AACJ,MAAA,OAAO,KAAA;AAAA,IACX;AAAA,EACJ;AAAA,EAEA,MAAM,UAAA,GAAgC;AAClC,IAAA,IAAI;AACA,MAAA,MAAM,WAAW,MAAM,KAAA,CAAM,CAAA,EAAG,IAAA,CAAK,OAAO,CAAA,SAAA,CAAW,CAAA;AACvD,MAAA,IAAI,CAAC,SAAS,EAAA,EAAI;AACd,QAAA,OAAO,EAAC;AAAA,MACZ;AACA,MAAA,MAAM,IAAA,GAAO,MAAM,QAAA,CAAS,IAAA,EAAK;AACjC,MAAA,OAAO,IAAA,CAAK,QAAQ,GAAA,CAAI,CAAC,MAAM,CAAA,CAAE,IAAI,KAAK,EAAC;AAAA,IAC/C,CAAA,CAAA,MAAQ;AACJ,MAAA,OAAO,EAAC;AAAA,IACZ;AAAA,EACJ;AAAA,EAEA,MAAM,QAAA,CAAS,QAAA,EAAqB,OAAA,EAAsD;AACtF,IAAA,IAAI;AACA,MAAA,MAAM,MAAA,GAAS,MAAM,YAAA,CAAa;AAAA,QAC9B,KAAA,EAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,OAAO,CAAA;AAAA,QAC/B,QAAA,EAAU,IAAA,CAAK,cAAA,CAAe,QAAQ,CAAA;AAAA,QACtC,aAAa,OAAA,EAAS,WAAA;AAAA,QACtB,WAAW,OAAA,EAAS,SAAA;AAAA,QACpB,eAAe,OAAA,EAAS;AAAA,OAC3B,CAAA;AAED,MAAA,OAAO;AAAA,QACH,SAAS,MAAA,CAAO,IAAA;AAAA,QAChB,KAAA,EAAO,OAAO,KAAA,GAAQ;AAAA,UAClB,YAAA,EAAc,OAAO,KAAA,CAAM,YAAA;AAAA,UAC3B,gBAAA,EAAkB,OAAO,KAAA,CAAM,gBAAA;AAAA,UAC/B,WAAA,EAAa,OAAO,KAAA,CAAM;AAAA,SAC9B,GAAI,KAAA,CAAA;AAAA,QACJ,cAAc,MAAA,CAAO;AAAA,OACzB;AAAA,IACJ,SAAS,KAAA,EAAO;AACZ,MAAA,MAAM,iBAAA,GAAqB,MAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA,IACtE,KAAA,CAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA;AAErD,MAAA,IAAI,iBAAA,EAAmB;AACnB,QAAA,MAAM,IAAI,aAAA;AAAA,UACN,CAAA,kDAAA,CAAA;AAAA,UACA,OAAA;AAAA,UACA,EAAE,KAAA,EAAO,OAAA,EAAS,IAAA,CAAK,OAAA;AAAQ,SACnC;AAAA,MACJ;AAEA,MAAA,MAAM,IAAI,aAAA;AAAA,QACN,CAAA,yBAAA,EAA6B,MAAgB,OAAO,CAAA,CAAA;AAAA,QACpD,OAAA;AAAA,QACA,EAAE,KAAA;AAAM,OACZ;AAAA,IACJ;AAAA,EACJ;AAAA,EAEA,OAAO,MAAA,CAAO,QAAA,EAAqB,OAAA,EAA4E;AAC3G,IAAA,IAAI;AACA,MAAA,MAAM,MAAA,GAAS,MAAM,UAAA,CAAW;AAAA,QAC5B,KAAA,EAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,OAAO,CAAA;AAAA,QAC/B,QAAA,EAAU,IAAA,CAAK,cAAA,CAAe,QAAQ,CAAA;AAAA,QACtC,aAAa,OAAA,EAAS,WAAA;AAAA,QACtB,WAAW,OAAA,EAAS;AAAA,OACvB,CAAA;AAED,MAAA,WAAA,MAAiB,KAAA,IAAS,OAAO,UAAA,EAAY;AACzC,QAAA,OAAA,EAAS,UAAU,KAAK,CAAA;AACxB,QAAA,MAAM,EAAE,OAAA,EAAS,KAAA,EAAO,IAAA,EAAM,KAAA,EAAM;AAAA,MACxC;AAEA,MAAA,MAAM,EAAE,OAAA,EAAS,EAAA,EAAI,IAAA,EAAM,IAAA,EAAK;AAAA,IACpC,SAAS,KAAA,EAAO;AACZ,MAAA,MAAM,iBAAA,GAAqB,MAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA,IACtE,KAAA,CAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA;AAErD,MAAA,IAAI,iBAAA,EAAmB;AACnB,QAAA,MAAM,IAAI,aAAA;AAAA,UACN,CAAA,kDAAA,CAAA;AAAA,UACA,OAAA;AAAA,UACA,EAAE,KAAA,EAAO,OAAA,EAAS,IAAA,CAAK,OAAA;AAAQ,SACnC;AAAA,MACJ;AAEA,MAAA,MAAM,IAAI,aAAA;AAAA,QACN,CAAA,wBAAA,EAA4B,MAAgB,OAAO,CAAA,CAAA;AAAA,QACnD,OAAA;AAAA,QACA,EAAE,KAAA;AAAM,OACZ;AAAA,IACJ;AAAA,EACJ;AAAA,EAEA,MAAM,iBAAA,CACF,QAAA,EACA,KAAA,EACA,OAAA,EACyB;AACzB,IAAA,IAAI;AACA,MAAA,MAAM,OAAA,GAAU,IAAA,CAAK,YAAA,CAAa,KAAK,CAAA;AAEvC,MAAA,MAAM,MAAA,GAAS,MAAM,YAAA,CAAa;AAAA,QAC9B,KAAA,EAAO,IAAA,CAAK,MAAA,CAAO,IAAA,CAAK,OAAO,CAAA;AAAA,QAC/B,QAAA,EAAU,IAAA,CAAK,cAAA,CAAe,QAAQ,CAAA;AAAA,QACtC,KAAA,EAAO,OAAA;AAAA,QACP,aAAa,OAAA,EAAS,WAAA;AAAA,QACtB,WAAW,OAAA,EAAS;AAAA,OACvB,CAAA;AAED,MAAA,OAAO;AAAA,QACH,SAAS,MAAA,CAAO,IAAA;AAAA,QAChB,SAAA,EAAW,MAAA,CAAO,SAAA,EAAW,GAAA,CAAI,CAAC,EAAA,MAAQ;AAAA,UACtC,IAAI,EAAA,CAAG,UAAA;AAAA,UACP,MAAM,EAAA,CAAG,QAAA;AAAA,UACT,WAAW,EAAA,CAAG;AAAA,SAClB,CAAE,CAAA;AAAA,QACF,KAAA,EAAO,OAAO,KAAA,GAAQ;AAAA,UAClB,YAAA,EAAc,OAAO,KAAA,CAAM,YAAA;AAAA,UAC3B,gBAAA,EAAkB,OAAO,KAAA,CAAM,gBAAA;AAAA,UAC/B,WAAA,EAAa,OAAO,KAAA,CAAM;AAAA,SAC9B,GAAI,KAAA,CAAA;AAAA,QACJ,cAAc,MAAA,CAAO;AAAA,OACzB;AAAA,IACJ,SAAS,KAAA,EAAO;AACZ,MAAA,MAAM,iBAAA,GAAqB,MAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA,IACtE,KAAA,CAAgB,OAAA,EAAS,QAAA,CAAS,cAAc,CAAA;AAErD,MAAA,IAAI,iBAAA,EAAmB;AACnB,QAAA,MAAM,IAAI,aAAA;AAAA,UACN,CAAA,kDAAA,CAAA;AAAA,UACA,OAAA;AAAA,UACA,EAAE,KAAA,EAAO,OAAA,EAAS,IAAA,CAAK,OAAA;AAAQ,SACnC;AAAA,MACJ;AAEA,MAAA,MAAM,IAAI,aAAA;AAAA,QACN,CAAA,8BAAA,EAAkC,MAAgB,OAAO,CAAA,CAAA;AAAA,QACzD,OAAA;AAAA,QACA,EAAE,KAAA;AAAM,OACZ;AAAA,IACJ;AAAA,EACJ;AAAA,EAEQ,eAAe,QAAA,EAAwF;AAC3G,IAAA,OAAO,QAAA,CAAS,GAAA,CAAI,CAAC,CAAA,MAAO;AAAA,MACxB,IAAA,EAAM,CAAA,CAAE,IAAA,KAAS,MAAA,GAAS,cAAc,CAAA,CAAE,IAAA;AAAA,MAC1C,SAAS,CAAA,CAAE;AAAA,KACf,CAAE,CAAA;AAAA,EACN;AAAA,EAEQ,aAAa,KAAA,EAA2D;AAC5E,IAAA,MAAM,SAAkD,EAAC;AAEzD,IAAA,KAAA,MAAW,KAAK,KAAA,EAAO;AACnB,MAAA,MAAA,CAAO,CAAA,CAAE,IAAI,CAAA,GAAI,IAAA,CAAK;AAAA,QAClB,aAAa,CAAA,CAAE,WAAA;AAAA,QACf,UAAA,EAAY,CAAA,CAAE,MAAA,CAAO,CAAA,CAAE,WAA2C;AAAA,OACrE,CAAA;AAAA,IACL;AAEA,IAAA,OAAO,MAAA;AAAA,EACX;AACJ","file":"chunk-6DMJN72X.js","sourcesContent":["import { createOllama } from \"ollama-ai-provider\";\r\nimport { generateText, streamText, tool } from \"ai\";\r\nimport { z } from \"zod\";\r\nimport { getConfig, type Message, type MCPTool, ProviderError } from \"@repo/shared\";\r\nimport { BaseProvider, type GenerateOptions, type StreamOptions, type ProviderResponse } from \"./base\";\r\n\r\nexport class LocalProvider extends BaseProvider {\r\n    readonly type = \"local\" as const;\r\n    readonly modelId: string;\r\n    private client: ReturnType<typeof createOllama>;\r\n    private baseUrl: string;\r\n\r\n    constructor(options?: { baseUrl?: string; modelId?: string }) {\r\n        super();\r\n        const config = getConfig();\r\n\r\n        this.baseUrl = options?.baseUrl ?? config.ollamaBaseUrl;\r\n        this.client = createOllama({ baseURL: this.baseUrl });\r\n        this.modelId = options?.modelId ?? config.ollamaModel;\r\n    }\r\n\r\n    async isAvailable(): Promise<boolean> {\r\n        try {\r\n            const response = await fetch(`${this.baseUrl}/api/tags`);\r\n            return response.ok;\r\n        } catch {\r\n            return false;\r\n        }\r\n    }\r\n\r\n    async listModels(): Promise<string[]> {\r\n        try {\r\n            const response = await fetch(`${this.baseUrl}/api/tags`);\r\n            if (!response.ok) {\r\n                return [];\r\n            }\r\n            const data = await response.json() as { models?: Array<{ name: string }> };\r\n            return data.models?.map((m) => m.name) ?? [];\r\n        } catch {\r\n            return [];\r\n        }\r\n    }\r\n\r\n    async generate(messages: Message[], options?: GenerateOptions): Promise<ProviderResponse> {\r\n        try {\r\n            const result = await generateText({\r\n                model: this.client(this.modelId),\r\n                messages: this.formatMessages(messages),\r\n                temperature: options?.temperature,\r\n                maxTokens: options?.maxTokens,\r\n                stopSequences: options?.stopSequences,\r\n            });\r\n\r\n            return {\r\n                content: result.text,\r\n                usage: result.usage ? {\r\n                    promptTokens: result.usage.promptTokens,\r\n                    completionTokens: result.usage.completionTokens,\r\n                    totalTokens: result.usage.totalTokens,\r\n                } : undefined,\r\n                finishReason: result.finishReason,\r\n            };\r\n        } catch (error) {\r\n            const isConnectionError = (error as Error).message?.includes(\"ECONNREFUSED\") ||\r\n                (error as Error).message?.includes(\"fetch failed\");\r\n\r\n            if (isConnectionError) {\r\n                throw new ProviderError(\r\n                    `Ollama is not running. Start it with: ollama serve`,\r\n                    \"local\",\r\n                    { error, baseUrl: this.baseUrl }\r\n                );\r\n            }\r\n\r\n            throw new ProviderError(\r\n                `Local generation failed: ${(error as Error).message}`,\r\n                \"local\",\r\n                { error }\r\n            );\r\n        }\r\n    }\r\n\r\n    async *stream(messages: Message[], options?: StreamOptions): AsyncIterable<{ content: string; done: boolean }> {\r\n        try {\r\n            const result = await streamText({\r\n                model: this.client(this.modelId),\r\n                messages: this.formatMessages(messages),\r\n                temperature: options?.temperature,\r\n                maxTokens: options?.maxTokens,\r\n            });\r\n\r\n            for await (const chunk of result.textStream) {\r\n                options?.onToken?.(chunk);\r\n                yield { content: chunk, done: false };\r\n            }\r\n\r\n            yield { content: \"\", done: true };\r\n        } catch (error) {\r\n            const isConnectionError = (error as Error).message?.includes(\"ECONNREFUSED\") ||\r\n                (error as Error).message?.includes(\"fetch failed\");\r\n\r\n            if (isConnectionError) {\r\n                throw new ProviderError(\r\n                    `Ollama is not running. Start it with: ollama serve`,\r\n                    \"local\",\r\n                    { error, baseUrl: this.baseUrl }\r\n                );\r\n            }\r\n\r\n            throw new ProviderError(\r\n                `Local streaming failed: ${(error as Error).message}`,\r\n                \"local\",\r\n                { error }\r\n            );\r\n        }\r\n    }\r\n\r\n    async generateWithTools(\r\n        messages: Message[],\r\n        tools: MCPTool[],\r\n        options?: GenerateOptions\r\n    ): Promise<ProviderResponse> {\r\n        try {\r\n            const aiTools = this.convertTools(tools);\r\n\r\n            const result = await generateText({\r\n                model: this.client(this.modelId),\r\n                messages: this.formatMessages(messages),\r\n                tools: aiTools,\r\n                temperature: options?.temperature,\r\n                maxTokens: options?.maxTokens,\r\n            });\r\n\r\n            return {\r\n                content: result.text,\r\n                toolCalls: result.toolCalls?.map((tc) => ({\r\n                    id: tc.toolCallId,\r\n                    name: tc.toolName,\r\n                    arguments: tc.args as Record<string, unknown>,\r\n                })),\r\n                usage: result.usage ? {\r\n                    promptTokens: result.usage.promptTokens,\r\n                    completionTokens: result.usage.completionTokens,\r\n                    totalTokens: result.usage.totalTokens,\r\n                } : undefined,\r\n                finishReason: result.finishReason,\r\n            };\r\n        } catch (error) {\r\n            const isConnectionError = (error as Error).message?.includes(\"ECONNREFUSED\") ||\r\n                (error as Error).message?.includes(\"fetch failed\");\r\n\r\n            if (isConnectionError) {\r\n                throw new ProviderError(\r\n                    `Ollama is not running. Start it with: ollama serve`,\r\n                    \"local\",\r\n                    { error, baseUrl: this.baseUrl }\r\n                );\r\n            }\r\n\r\n            throw new ProviderError(\r\n                `Local tool generation failed: ${(error as Error).message}`,\r\n                \"local\",\r\n                { error }\r\n            );\r\n        }\r\n    }\r\n\r\n    private formatMessages(messages: Message[]): Array<{ role: \"user\" | \"assistant\" | \"system\"; content: string }> {\r\n        return messages.map((m) => ({\r\n            role: m.role === \"tool\" ? \"assistant\" : m.role,\r\n            content: m.content,\r\n        }));\r\n    }\r\n\r\n    private convertTools(tools: MCPTool[]): Record<string, ReturnType<typeof tool>> {\r\n        const result: Record<string, ReturnType<typeof tool>> = {};\r\n\r\n        for (const t of tools) {\r\n            result[t.name] = tool({\r\n                description: t.description,\r\n                parameters: z.object(t.inputSchema as Record<string, z.ZodTypeAny>),\r\n            });\r\n        }\r\n\r\n        return result;\r\n    }\r\n}\r\n"]}